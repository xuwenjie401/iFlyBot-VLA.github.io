<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="iFlyBot-VLA - iFlyTek Research & Development Groups, Robotics Team">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="iFlyTek Reserch & Development Groups">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="iFlyBot-VLA">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>iFlyBot-VLA Tech Report</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="assets/images/iflytek_logo-1.png">
  <link rel="apple-touch-icon" href="assets/images/iflytek_logo-1.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">iFlyBot-VLA</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  iFlyTek Research & Development Groups, LindenBot
                </span>
              </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/xuwenjie401/iFlyBot-VLA.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(coming soon)</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="assets\images\huggingface_logo-noborder.svg" alt="Hugging Face logo" style="width:1em; height:1em; vertical-align:middle;">
                  </span>
                  <span>Dataset(coming soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Replace with embedded YouTube video -->
      <!-- <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;"> -->
      <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"></div>
        <iframe 
          src="https://www.youtube.com/embed/JHgNqMMmx8U" 
          title="iFlyBot-VLA Demo"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen
          style="position:absolute; top:0; left:0; width:100%; height:100%;">
        </iframe>
      </div>

      <!-- Video description -->
      <h2 class="subtitle has-text-centered" style="margin-top: 1em;">
        iFlyBot-VLA, a robust bimanual manipulation policy for diverse tasks
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework.
            The main contributions are listed as follows:
            (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos;
            (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training;
            (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone.

            Specifically, the VLM is trained to predict two complementary forms of actions:
            latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and
            structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics.
            This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation.

            Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our framework, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks.
            Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <!-- 内容项1 -->
      <div class="item" style="margin-bottom: 8em;">
        <img src="assets/images/vla_pipeline.jpg" alt="Architecture visualization" 
             loading="lazy" style="display:block; margin:auto; max-width:90%; border-radius:12px;">
        <h2 class="subtitle has-text-centered" style="margin-top: 1.5em;">
          The architecture of iFlyBot-VLA consists primarily of a language transformer backbone and an action expert network.
          The model generates executable robot actions through a combination of explicit and implicit planning.
        </h2>
      </div>

      <!-- 内容项2 -->
      <div class="item" style="margin-bottom: 8em;">
        <img src="assets/images/lam_all-1.PNG" alt="Latent action model visualization"
             loading="lazy" style="display:block; margin:auto; max-width:90%; border-radius:12px;">
        <h2 class="subtitle has-text-centered" style="margin-top: 1.5em;">
          The previously mentioned latent actions are produced by a latent action model we pretrained following the LAPA-simIlar framework, employing a VQ-VAE pipeline.
        </h2>
      </div>

      <!-- 内容项3 -->
      <div class="item" style="margin-bottom: 8em;">
        <img src="assets\images\pretrain-data.PNG" alt="Experimental results visualization"
             loading="lazy" style="display:block; margin:auto; max-width:90%; border-radius:12px;">
        <h2 class="subtitle has-text-centered" style="margin-top: 1.5em;">
          Overview of the data composition and proportions employed in the pre-training stage.
        </h2>
      </div>

      <!-- 内容项4 -->
      <div class="item" style="margin-bottom: 8em;">
        <img src="assets/images/libero_exp1.PNG" alt="Experimental results visualization"
             loading="lazy" style="display:block; margin:auto; max-width:90%; border-radius:12px;">
        <h2 class="subtitle has-text-centered" style="margin-top: 1.5em;">
          Experimental results demonstrate that the iFlyBot-VLA model achieves strong performance in the Libero simulator.
        </h2>
      </div>

    </div>
  </div>
</section>



<!-- experiments -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Real-World Experiments</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-image1">
          <!-- TODO: Add poster image for better preview -->
          <img src="assets\images\gpp_robustness.PNG" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            We evaluated the performance of iFlyBot-VLA on a general pick-and-place task, testing its robustness under disturbances from unseen objects, lighting variations, and unseen environments.
          </h2>
        </div>
        <div class="item item-image2">
            <!-- Your video file here -->
            <img src="assets\images\gpp_result_short.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              iFlyTek-VLA trained on our self-collected data achieves a higher real-world success rate compared to π₀.
            </h2>
        </div>
        <div class="item item-image3">
            <!-- Your video file here -->
            <img src="assets\images\long-horizon.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
             iFlyTek-VLA demonstrates outstanding performance in complex long-horizon, dual-arm manipulation tasks within a simulated factory assembly-line environment.
            </h2>
        </div>
        <div class="item item-image4">
            <!-- Your video file here -->
            <img src="assets\images\folding1.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Folding clothes that are randomly placed or partially unfolded is a highly challenging task for VLAs, requiring both precise execution and strong robustness.
              In real-world experiments, iFlyBot-VLA demonstrates remarkable robustness in performing such tasks.
            </h2>
        </div>
        <div class="item item-image4">
            <!-- Your video file here -->
            <img src="assets\images\folding_eval_3.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Given the complexity of folding task, we provide a more detailed comparison. 
              Since locating the correct grasping points often requires multiple attempts, we imposed a 3-minute time limit for each full execution. The detailed results are presented in the bottom image, 
              where the x-axis corresponds to the steps illustrated in the upper image.
            </h2>
        </div>
        
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">
        More From Our Team:<br><br>
        <span class="has-text-centered" style="display:block;">iFlyBot-VLM</span>
      </h2>

      <div class="publication-links is-flex is-justify-content-center" style="gap: 1rem; margin-top: 1rem;">
        <span class="link-block">
          <a href="https://huggingface.co/IflyBot/IflyBotVLM" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="assets/images/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="width:1em; height:1em; vertical-align:middle;">
            </span>
            <span>iFlyBot-VLM</span>
          </a>
        </span>

        <span class="link-block">
          <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>

      <section class="hero teaser" style="margin-top: 3rem;">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <!-- Replace with embedded YouTube video -->
            <!-- <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;"> -->
            <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"></div>
              <iframe 
                src="https://www.youtube.com/embed/bw4n0k-2yBI?rel=0"
                title="iFlyBot-VLM Demo"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                allowfullscreen
                style="position:absolute; top:0; left:0; width:100%; height:100%;">
              </iframe>
            </div>
            <h2 class="subtitle has-text-centered">
              iFlyBot-VLM trajectory inference demonstration.
            </h2>
          </div>
        </div>
      </section>
      <section class="section hero is-light">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <!-- TODO: Replace with your paper abstract -->
                <p>
                We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) developed to advance the domain of embodied intelligence. The central objective of iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control. To this end, the model abstracts complex visual and spatial information into a body-agnostic and transferable “Operational Language,” enabling seamless perception–action closed-loop coordination across diverse robotic platforms.
                The architecture of iFlyBot-VLM is systematically designed to realize four key functional capabilities essential for embodied intelligence:
                1) Spatial understanding and metric reasoning;
                2) Interactive target grounding;
                3) Action abstraction and control parameter generation;
                4) Task planning and skill sequencing.
                We envision iFlyBot-VLM as a scalable and generalizable foundation model for embodied AI, facilitating the progression from specialized, task-oriented systems toward generalist, cognitively capable agents. We conducted evaluations on ten mainstream embodied intelligence–related VLM benchmark datasets, such as Blink and Where2Place, achieving state-of-the-art performance while preserving the model’s generalization capabilities. Both the training data and model weights will be publicly released to foster further research and development in the field of embodied intelligence.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-image0">
          <!-- TODO: Add poster image for better preview -->
          <img src="assets\images\architecture.png" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
             iFlyBot-VLM inherits the robust, three-stage "ViT-Projector-LLM" paradigm from established Vision-Language Models. It integrates a dedicated, incrementally pre-trained Visual Encoder with an advanced Language Model via a simple, randomly initialized MLP projector for efficient feature alignment.
          </h2>
        </div>
        <div class="item item-image1">
          <!-- TODO: Add poster image for better preview -->
          <img src="assets\images\smart_donut_chart.png" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
              The rich composition of embodied AI domain data has significantly enhanced the performance of iFlyBot-VLM in spatial understanding, perception, and task planning.
          </h2>
        </div>
        <div class="item item-image2">
            <!-- Your video file here -->
            <img src="assets\images\radar_performance.png" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              iFlyBot-VLM achieves state-of-the-art (SOTA) or near-SOTA performance on spatial understanding, perception, and task planning benchmarks.
            </h2>
        </div>
        <div class="item item-image3">
            <!-- Your video file here -->
            <img src="assets\images\benchmark_performance.jpg" alt="Second research result visualization" loading="lazy"/>
        </div>
      </div>
    </div>
  </div>
</section>

        
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">iFlyBot-VLM in actions</h2>
      <div id="results-carousel" class="carousel results-carousel">

  <!-- 视频1 -->
  <div class="item item-video1">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
    <!-- <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"> -->
      <iframe 
        src="https://www.youtube.com/embed/uK9YomfbPjM?rel=0"
        title="iFlyBot-VLM Demo"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      iFlyBot-VLM Applied to Object Picking and Placing
    </h2>
  </div>

  <!-- 视频2 -->
  <div class="item item-video2">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
    <!-- <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"> -->
      <iframe 
        src="https://www.youtube.com/embed/Tjgj8fHqE70?rel=0"
        title="General pick-and-place"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      iFlybot-VLM: Enabling Spatial Understanding-Enhanced Instruction Generalization
    </h2>
  </div>

  <!-- 视频3 -->
  <div class="item item-video3">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
    <!-- <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"> -->
      <iframe 
        src="https://www.youtube.com/embed/gSYQUGhC9mc?rel=0"
        title="Long-horizon manipulation"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      iFlybot-VLM: Driving Advanced Object Generalization
    </h2>
  </div>

  <!-- 视频4 -->
  <div class="item item-video4">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
    <!-- <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"> -->
      <iframe 
        src="https://www.youtube.com/embed/PWyzK0kcxiA?rel=0"
        title="Cloth folding task"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      iFlybot-VLM: Boosting Advanced Scene Generalization
    </h2>
  </div>

</div>

<!-- 注解部分 -->
<div class="has-text" style="margin-top: 0em;">
  <p class="is-size-7" style="color:#555; font-style:italic;">
    <strong style="color:#d67f00;">*</strong> Motion: Supported by Curobo
  </p>
  <p class="is-size-7" style="color:#555; font-style:italic;">
    <strong style="color:#d67f00;">*</strong>  Interaction: Supported by iFlyTek Multimodal Interaction Backpack
  </p>
</div>


    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
