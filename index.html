<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="iFlyBot-VLA - iFlyTek Research & Development Groups, Robotics Team">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="iFlyTek Reserch & Development Groups">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="iFlyBot-VLA">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>iFlyBot-VLA Tech Report</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="assets/images/iflytek_logo-1.png">
  <link rel="apple-touch-icon" href="assets/images/iflytek_logo-1.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">iFlyBot-VLA</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  iFlyTek Research & Development Groups Robotics Team<sup>*</sup>,
                </span>
              </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/xuwenjie401/iFlyBot-VLA.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(coming soon)</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="assets\images\huggingface_logo-noborder.svg" alt="Hugging Face logo" style="width:1em; height:1em; vertical-align:middle;">
                  </span>
                  <span>Dataset(coming soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Replace with embedded YouTube video -->
      <!-- <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;"> -->
      <div style="width:1280px; height:450px; border-radius:12px; overflow:hidden;"></div>
        <iframe 
          src="https://www.youtube.com/embed/JHgNqMMmx8U" 
          title="iFlyBot-VLA Demo"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen
          style="position:absolute; top:0; left:0; width:100%; height:100%;">
        </iframe>
      </div>

      <!-- Video description -->
      <h2 class="subtitle has-text-centered" style="margin-top: 1em;">
        iFlyBot-VLA, a robust bimanual manipulation policy for diverse tasks
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework.
            The main contributions are listed as follows:
            (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos;
            (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training;
            (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone.

            Specifically, the VLM is trained to predict two complementary forms of actions:
            latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and
            structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics.
            This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation.

            Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our framework, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks.
            Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="assets\images\vla_pipeline.jpg" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          The architecture of iFlyBot-VLA consists primarily of a language transformer backbone and an action expert network.
          The model generates executable robot actions through a combination of explicit and implicit planning.
          The key–value (KV) cache from the VLM component is passed to the downstream action expert, while the FAST token—which corresponds to the implicit planning process—is not forwarded to the Action Expert.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="assets\images\lam_all-1.PNG" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          We pretrained a latent action model following the LAPA framework.
          Specifically, the model adopts a VQ-VAE pipeline, where the encoder compresses the action representation from pairs of sequential images, and the decoder reconstructs future frames to construct the training loss.
          Compared to UniVLA or LAPA, we further extend the dataset with a large amount of bimanual (dual-arm) manipulation data, and accordingly adjust the codebook capacity and related parameters.
        </h2>
      </div>

     <div class="item">
      <!-- Your image here -->
      <img src="assets\images\libero_exp1.PNG" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Experimental results demonstrate that the iFlyBot-VLA model achieves promising performance in the Libero simulator, outperforming existing open-source models and showing a significant improvement over current action-chunk-based inference approaches.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Real-World Experiments</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-image1">
          <!-- TODO: Add poster image for better preview -->
          <img src="assets\images\gpp_robustness.PNG" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            We evaluated the performance of iFlyBot-VLA on a general pick-and-place task, testing its robustness under disturbances from unseen objects, lighting variations, and unseen environments.
          </h2>
        </div>
        <div class="item item-image2">
            <!-- Your video file here -->
            <img src="assets\images\general_pick_place-1.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              iFlyTek-VLA trained on our self-collected data achieves a higher real-world success rate compared to π₀.
            </h2>
        </div>
        <div class="item item-image3">
            <!-- Your video file here -->
            <img src="assets\images\long-horizon.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
             iFlyTek-VLA demonstrates outstanding performance in complex long-horizon, dual-arm manipulation tasks within a simulated factory assembly-line environment.
            </h2>
        </div>
        <div class="item item-image4">
            <!-- Your video file here -->
            <img src="assets\images\folding1.PNG" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              Folding clothes that are randomly placed or partially unfolded is a highly challenging task for VLAs, requiring both precise execution and strong robustness.
              In real-world experiments, iFlyBot-VLA demonstrates remarkable robustness in performing such tasks.
            </h2>
        </div>

        
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">
        More From Our Team:<br><br>
        <span class="has-text-centered" style="display:block;">iFlyBot-VLM</span>
      </h2>

      <div class="has-text-centered" style="margin-top:1rem;">
        <a href="https://huggingface.co/IflyBot/IflyBotVLM" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <img src="assets\images\huggingface_logo-noborder.svg" alt="Hugging Face logo" style="width:1em; height:1em; vertical-align:middle;">
          </span>
          <span>iFlyBot-VLM</span>
        </a>
      </div>

      <section class="hero teaser">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <!-- Replace with embedded YouTube video -->
            <!-- <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;"> -->
            <div style="width:800px; height:450px; border-radius:12px; overflow:hidden;">
              <iframe 
                src="https://www.youtube.com/embed/uK9YomfbPjM" 
                title="iFlyBot-VLM Demo"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                allowfullscreen
                style="position:absolute; top:0; left:0; width:100%; height:100%;">
              </iframe>
            </div>
            <!-- Video description -->
            <h2 class="subtitle has-text-centered" style="margin-top: 1em;">
              iFlyBot-VLM, trajectory inference demonstration
            </h2>
          </div>
        </div>
      </section>
      <section class="section hero is-light">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <!-- TODO: Replace with your paper abstract -->
                <p>
                We present <b>iFlyBot-VLM</b>, a general-purpose Vision-Language Model (VLM) explicitly designed for the domain of Embodied Intelligence. The central objective of iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control. To this end, the model abstracts complex visual and spatial information into a body-agnostic and transferable "Operational Language", thereby enabling seamless perception–action closed-loop coordination across diverse robotic platforms.
                The architecture of <b>iFlyBot-VLM</b> is systematically designed to realize four key functional capabilities essential for embodied intelligence:
                1) Spatial Understanding and Metric Reasoning;
                2) Interactive Target Grounding;
                3) Action Abstraction and Control Parameter Generation;
                4) Task Planning and Skill Sequencing;
                
                We envision <b>iFlyBot-VLM</b> as a scalable and generalizable foundation model for embodied AI, facilitating the progression from specialized task-oriented systems toward generalist, cognitively capable agents.  
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-image0">
          <!-- TODO: Add poster image for better preview -->
          <img src="assets\images\architecture.png" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
             iFlyBot-VLM inherits the robust, three-stage "ViT-Projector-LLM" paradigm from established Vision-Language Models. It integrates a dedicated, incrementally pre-trained Visual Encoder with an advanced Language Model via a simple, randomly initialized MLP projector for efficient feature alignment.
          </h2>
        </div>
        <div class="item item-image1">
          <!-- TODO: Add poster image for better preview -->
          <img src="assets\images\smart_donut_chart.png" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
              The rich composition of embodied AI domain data has significantly enhanced the performance of iFlyBot-VLM in spatial understanding, perception, and task planning.
          </h2>
        </div>
        <div class="item item-image2">
            <!-- Your video file here -->
            <img src="assets\images\radar_performance.png" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              iFlyBot-VLM achieves state-of-the-art (SOTA) or near-SOTA performance on spatial understanding, perception, and task planning benchmarks.
            </h2>
        </div>
        <div class="item item-image3">
            <!-- Your video file here -->
            <img src="assets\images\benchmark_performance.jpg" alt="Second research result visualization" loading="lazy"/>
        </div>
      </div>
    </div>
  </div>
</section>

        
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">More Demostrations</h2>
      <div id="results-carousel" class="carousel results-carousel">

  <!-- 视频1 -->
  <div class="item item-video1">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
      <iframe 
        src="https://www.youtube.com/embed/bw4n0k-2yBI?rel=0"
        title="iFlyBot-VLM Demo"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      iFlyBot-VLM trajectory inference demonstration.
    </h2>
  </div>

  <!-- 视频2 -->
  <div class="item item-video2">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
      <iframe 
        src="https://www.youtube.com/embed/Tjgj8fHqE70?rel=0"
        title="General pick-and-place"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      Performance of iFlyBot-VLA in general pick-and-place task.
    </h2>
  </div>

  <!-- 视频3 -->
  <div class="item item-video3">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
      <iframe 
        src="https://www.youtube.com/embed/gSYQUGhC9mc?rel=0"
        title="Long-horizon manipulation"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      iFlyBot-VLA in long-horizon, dual-arm tasks.
    </h2>
  </div>

  <!-- 视频4 -->
  <div class="item item-video4">
    <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden; border-radius:12px;">
      <iframe 
        src="https://www.youtube.com/embed/PWyzK0kcxiA?rel=0"
        title="Cloth folding task"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen
        style="position:absolute; top:0; left:0; width:100%; height:100%;">
      </iframe>
    </div>
    <h2 class="subtitle has-text-centered">
      Folding clothes — robustness under deformation.
    </h2>
  </div>

</div>

    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
